{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import psycopg2\n",
    "import os\n",
    "import datetime\n",
    "import multiprocessing\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deck_card_counts(schema):\n",
    "    '''\n",
    "    Gets the deck data needed for the Spark ALS model.\n",
    "\n",
    "    INPUT:\n",
    "        - schema: StructType object, schema for spark ratings df\n",
    "\n",
    "    OUTPUT:\n",
    "        - incomplete_ratings: Spark df, ratings for all deck-card combos found in real decks\n",
    "    '''\n",
    "\n",
    "    cursor.execute('SELECT deck_id, cardstorm_id, card_count FROM decks')\n",
    "\n",
    "    incomplete_ratings = spark.createDataFrame(data=cursor.fetchall(),\n",
    "                                               schema=schema)\n",
    "\n",
    "    return incomplete_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unused_cardstorm_ids():\n",
    "    '''\n",
    "    Gets a list of all the unused cardstorm_ids from the db\n",
    "\n",
    "    INPUT:\n",
    "        NONE\n",
    "\n",
    "    OUTPUT:\n",
    "        - unused_ids: list of ints, all unused cardstorm_ids.\n",
    "    '''\n",
    "\n",
    "    cursor.execute('''SELECT cardstorm_id\n",
    "                      FROM cards\n",
    "                      WHERE cardstorm_id\n",
    "                        NOT IN (SELECT DISTINCT cardstorm_id FROM decks)''')\n",
    "\n",
    "    unused_ids = [_[0] for _ in cursor.fetchall()]\n",
    "\n",
    "    return unused_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_unused_cardstorm_ids(unused_ids):\n",
    "    '''\n",
    "    Creates fake date for all unused cardstorm_ids.\n",
    "\n",
    "    INPUT:\n",
    "        - unused_ids: list of ints, all cardstorm_ids that don't show up in a deck\n",
    "\n",
    "    OUTPUT:\n",
    "        - filler_data: list of tuples, deck_id - cardstorm_id - card_count for\n",
    "                        each of the unused_ids. deck_id is '-1' to easily id them\n",
    "    '''\n",
    "\n",
    "    filler_data = []\n",
    "    for unused_id in unused_ids:\n",
    "        filler_data.append((-1, unused_id, 1))\n",
    "\n",
    "    return filler_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_product_rdd(product_rdd):\n",
    "    '''\n",
    "    Adds the product features rdd from the spark ALS model to the db.\n",
    "\n",
    "    INPUT:\n",
    "        - product_rdd: Spark rdd, features pulled from fitted Spark ALS model\n",
    "\n",
    "    OUTPUT:\n",
    "        - success: bool, True if no problems were encountered.\n",
    "    '''\n",
    "    current_date = str(datetime.date.today())\n",
    "\n",
    "    cursor.execute('SELECT MAX(run_id) FROM product_matrices')\n",
    "    run_id = cursor.fetchone()[0]\n",
    "    if run_id is None:\n",
    "        run_id = 0\n",
    "    run_id += 1\n",
    "\n",
    "    for cardstorm_id, features in product_rdd.collect():\n",
    "        query = '''INSERT INTO product_matrices (cardstorm_id, features, date, run_id)\n",
    "                   VALUES (%s, %s, %s, %s)'''\n",
    "\n",
    "        try:\n",
    "            cursor.execute(query, vars=[cardstorm_id, features, current_date, run_id])\n",
    "        except psycopg2.IntegrityError:\n",
    "            return False\n",
    "            continue\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_recommender():\n",
    "    '''\n",
    "    Makes the recommender model! Gets deck data from the database, makes filler\n",
    "    data for unused cards, uses Spark ALS to train a model of implicit ratings.\n",
    "    Pulls out the product features matrix (often referred to as V) and\n",
    "    uploads it to the database with the current data attached.\n",
    "\n",
    "    INPUT:\n",
    "        NONE\n",
    "\n",
    "    OUTPUT:\n",
    "        NONE\n",
    "\n",
    "    Does everything\n",
    "\n",
    "        Get deck data from db\n",
    "        create schema for spark DF\n",
    "        get unused cardstorm_ids\n",
    "        create fake data for all unused cards\n",
    "        make new spark df from unused cards\n",
    "        merge dataframes\n",
    "        create and train ALS implicit model\n",
    "        get the product matrix\n",
    "        upload df to db\n",
    "    '''\n",
    "\n",
    "    ratings_schema = StructType([StructField('deck_id', IntegerType()),\n",
    "                                 StructField('cardstorm_id', IntegerType()),\n",
    "                                 StructField('card_count', IntegerType())])\n",
    "\n",
    "    incomplete_ratings = get_deck_card_counts(schema=ratings_schema)\n",
    "\n",
    "    unused_ids = get_unused_cardstorm_ids()\n",
    "\n",
    "    filler_data = fill_unused_cardstorm_ids(unused_ids)\n",
    "\n",
    "    filler_ratings = spark.createDataFrame(data=filler_data,\n",
    "                                           schema=ratings_schema)\n",
    "    ratings_df = incomplete_ratings.union(filler_ratings)\n",
    "    \n",
    "    model = ALS(rank=30, implicitPrefs=True)\n",
    "    \n",
    "    model.fit()\n",
    "\n",
    "#     model = ALS.trainImplicit(ratings=ratings_df, rank=30)\n",
    "\n",
    "    product_rdd = model.productFeatures()\n",
    "\n",
    "    upload_status = upload_product_rdd(product_rdd)\n",
    "\n",
    "    if upload_status: conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    global dbname, host, username, password, conn, cursor, spark\n",
    "    dbname = os.environ['CARDSTORM_DB_DBNAME']\n",
    "    host = os.environ['CARDSTORM_DB_HOST']\n",
    "    username = os.environ['CARDSTORM_DB_USERNAME']\n",
    "    password = os.environ['CARDSTORM_DB_PASSWORD']\n",
    "\n",
    "\n",
    "    conn = psycopg2.connect('dbname={} host={} user={} password={}'.format(dbname, host, username, password))\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    spark = (ps.sql.SparkSession.builder\n",
    "                   .master('local[{}]'.format(multiprocessing.cpu_count()))\n",
    "                   .appName('cardstorm modeling')\n",
    "                   .getOrCreate())\n",
    "\n",
    "    make_recommender()\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global dbname, host, username, password, conn, cursor, spark\n",
    "dbname = os.environ['CARDSTORM_DB_DBNAME']\n",
    "host = os.environ['CARDSTORM_DB_HOST']\n",
    "username = os.environ['CARDSTORM_DB_USERNAME']\n",
    "password = os.environ['CARDSTORM_DB_PASSWORD']\n",
    "\n",
    "\n",
    "conn = psycopg2.connect('dbname={} host={} user={} password={}'.format(dbname, host, username, password))\n",
    "cursor = conn.cursor()\n",
    "\n",
    "spark = (ps.sql.SparkSession.builder\n",
    "               .master('local[{}]'.format(multiprocessing.cpu_count()))\n",
    "               .appName('cardstorm modeling')\n",
    "               .getOrCreate())\n",
    "\n",
    "# make_recommender()\n",
    "\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings_schema = StructType([StructField('deck_id', IntegerType()),\n",
    "                                 StructField('cardstorm_id', IntegerType()),\n",
    "                                 StructField('card_count', IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incomplete_ratings = get_deck_card_counts(schema=ratings_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unused_ids = get_unused_cardstorm_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filler_data = fill_unused_cardstorm_ids(unused_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filler_ratings = spark.createDataFrame(data=filler_data, schema=ratings_schema)\n",
    "ratings_df = incomplete_ratings.union(filler_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ALS(rank=30, implicitPrefs=True, userCol='deck_id', itemCol='cardstorm_id', ratingCol='card_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = model.fit(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     model = ALS.trainImplicit(ratings=ratings_df, rank=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fitted_model.itemFactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute('SELECT ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_rdd = fitted_model.itemFactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_status = upload_product_rdd(product_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if upload_status: conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
